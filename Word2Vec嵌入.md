## Word2Vec åµŒå…¥

### èƒŒæ™¯
word2vecæ˜¯Googleåœ¨2013å¹´æ¨å‡ºå¼€æºçš„NLPå·¥å…·ã€‚å®ƒçš„ç‰¹ç‚¹å°±æ˜¯å°†æ‰€æœ‰çš„è¯å‘é‡åŒ–ï¼Œè¿™æ ·è¯ä¸è¯ä¹‹é—´å°±å¯ä»¥å®šé‡çš„å»åº¦é‡å®ƒä»¬ä¹‹é—´çš„å…³ç³»ï¼ŒæŒ–æ˜è¯ä¹‹é—´çš„è”ç³»

### ç®—æ³•
Word2Vecçš„ç®—æ³•åŒ…æ‹¬ skip-gram å’Œ cbow æ¨¡å‹ï¼Œä½¿ç”¨åˆ†å±‚SOFTMAXæˆ–è´Ÿé‡‡æ ·

### åµŒå…¥æ–¹æ³•
åœ¨Gensimä¸­è®­ç»ƒå•è¯å‘é‡çš„æ–¹æ³•é™¤äº†Word2Vecã€‚è¿˜æœ‰[Doc2Vec](./Doc2Vec.md)ï¼Œ[FastText](./FastText.md)åŒ…è£…VarEmbedå’ŒWordRankã€‚

### ç”¨æ³•ç¤ºä¾‹
ä½¿ç”¨ç”¨ä¾‹ï¼Œåˆå§‹åŒ–æ¨¡å‹ï¼š
```python
from gensim.test.utils import common_texts
from gensim.models import Word2Vec
from multiprocessing import cpu_count

model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=cpu_count())
model.save('word2vec.model')
```
æµå¼ä¼ è¾“åŸ¹è®­ï¼Œæ„å‘³ç€å¥å­å¯ä»¥æ˜¯ç”Ÿæˆå™¨ï¼Œå³æ—¶ä»ç£ç›˜è¯»å–è¾“å…¥æ•°æ®ï¼Œè€Œæ— éœ€å°†æ•´ä¸ªè¯­æ–™åº“åŠ è½½åˆ°RAMä¸­ã€‚

è¿™ä¹Ÿæ„å‘³ç€æ‚¨å¯ä»¥åœ¨ä»¥åç»§ç»­è®­ç»ƒæ¨¡å‹ï¼š
```python
model = Word2Vec.load('word2vec.model')
model.train([["hello", "world"]], total_examples=1, epochs=1)
```
è®­ç»ƒè¿‡çš„å•è¯å‘é‡å­˜å‚¨åœ¨model.wvä¸­çš„KeyedVectorså®ä¾‹ä¸­ï¼š
```python
vector = model.wv['computer']
print(vector)
```

### å¸¸ç”¨æ–¹æ³•
class gensim.models.word2vec.LineSentenceï¼ˆsourceï¼Œmax_sentence_length = 10000ï¼Œlimit = None ï¼‰Â¶   
è¿­ä»£åŒ…å«å¥å­çš„æ–‡ä»¶ï¼šä¸€è¡Œæ˜¯ä¸€ä¸ªå¥å­ã€‚å•è¯å¿…é¡»å·²ç»è¿‡é¢„å¤„ç†å¹¶ç”±ç©ºæ ¼åˆ†éš”ã€‚
ä½œç”¨å°±æ˜¯æŠŠæ–‡æœ¬è½¬åŒ–ä¸ºåˆ—è¡¨ï¼Œçœ‹ä¸‹é¢çš„å°ä¾‹å­ï¼š
```python
from gensim.test.utils import datapath
from gensim.models.word2vec import LineSentence
sentences = LineSentence(datapath('lee_background.cor'))
for sentence in sentences:
    print(sentence)
```

gensim.models.word2vec.PathLineSentencesï¼ˆsourceï¼Œmax_sentence_length = 10000ï¼Œlimit = None ï¼‰Â¶  
å’ŒLineSentenceç±»ä¼¼ï¼Œä½†æŒ‰æ–‡ä»¶åçš„å­—æ¯é¡ºåºå¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶ã€‚
è¯¥ç›®å½•å¿…é¡»åªåŒ…å«å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è¯»å–çš„æ–‡ä»¶gensim.models.word2vec.LineSentenceï¼š.bz2ï¼Œ.gzå’Œæ–‡æœ¬æ–‡ä»¶ã€‚ä»»ä½•ä¸ä»¥.bz2æˆ–.gzç»“å°¾çš„æ–‡ä»¶éƒ½è¢«å‡å®šä¸ºæ–‡æœ¬æ–‡ä»¶ã€‚

gensim.models.word2vec.Word2Vec(sentences=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), max_final_vocab=None)Â¶  
> Word2Vecçš„å‚æ•°ï¼š
+ sentencesï¼šå¯è¿­ä»£çš„å¥å­å¯ä»¥ç®€å•åœ°æ˜¯ä»¤ç‰Œåˆ—è¡¨çš„åˆ—è¡¨ï¼Œä½†æ˜¯å¯¹äºæ›´å¤§çš„è¯­æ–™åº“ï¼Œè€ƒè™‘ç›´æ¥ä»ç£ç›˜/ç½‘ç»œæµå¼ä¼ è¾“å¥å­çš„è¿­ä»£ã€‚ æœ‰å…³æ­¤ç±»ç¤ºä¾‹ï¼Œè¯·å‚é˜…word2vecæ¨¡å—ä¸­çš„BrownCorpusï¼ŒText8Corpusæˆ–LineSentenceã€‚ å¦è¯·å‚é˜…Pythonä¸­çš„æ•°æ®æµæ•™ç¨‹ã€‚ å¦‚æœæ‚¨ä¸æä¾›å¥å­ï¼Œåˆ™æ¨¡å‹å°†ä¿æŒæœªåˆå§‹åŒ–çŠ¶æ€ - å¦‚æœæ‚¨è®¡åˆ’ä»¥å…¶ä»–æ–¹å¼åˆå§‹åŒ–è¯¥æ¨¡å‹ï¼Œè¯·ä½¿ç”¨è¯¥æ¨¡å‹ã€‚
+ sizeï¼šå•è¯å‘é‡çš„ç»´åº¦
+ windowï¼šå¥å­ä¸­å½“å‰å’Œé¢„æµ‹å•è¯ä¹‹é—´çš„æœ€å¤§è·ç¦»
+ min_countï¼šå¿½ç•¥æ€»é¢‘ç‡ä½äºæ­¤å€¼çš„æ‰€æœ‰å•è¯
+ workersï¼šä½¿ç”¨è¿™äº›å·¥ä½œçº¿ç¨‹æ¥è®­ç»ƒæ¨¡å‹ï¼ˆ=ä½¿ç”¨å¤šæ ¸æœºå™¨è¿›è¡Œæ›´å¿«çš„è®­ç»ƒï¼‰
+ sgï¼šè®­ç»ƒçš„æ–¹æ³•ï¼š1ä¸ºskip-gram,2ä¸ºcbow
+ hsï¼šå¦‚æœä¸º1ï¼Œåˆ†å±‚softmaxå°†ç”¨äºæ¨¡å‹è®­ç»ƒã€‚å¦‚æœä¸º0ï¼Œnegativeä¸ä¸º0ï¼Œåˆ™é‡‡ç”¨è´Ÿé‡‡æ ·
+ negativeï¼šå¦‚æœå¤§äº0ï¼Œå°†é‡‡ç”¨è´Ÿé‡‡æ ·ï¼Œnegativeè¡¨ç¤ºåº”ç»˜åˆ¶å¤šå°‘â€œå™ªå£°è¯â€ï¼ˆé€šå¸¸åœ¨5-20ä¹‹é—´ï¼‰ã€‚å¦‚æœè®¾ä¸º0ï¼Œåˆ™ä¸ä½¿ç”¨è´Ÿé‡‡æ ·

ğŸŒ° ä¾‹å­
åˆå§‹åŒ–å¹¶è®­ç»ƒWord2Vecæ¨¡å‹
```python
from gensim.models import Word2Vec
sentence = [["cat", "say", "meow"], ["dog", "say", "woof"]]
model = Word2Vec(sentence, min_count=1)
print(model)
```

### æ¨¡å‹çš„å¸¸ç”¨æ–¹æ³•
> most_similar(positive = Noneï¼Œnegative = Noneï¼Œtopn = 10ï¼Œrestrict_vocab = Noneï¼Œindexer = None ï¼‰Â¶  

æ‰¾åˆ°å‰Nä¸ªæœ€ç›¸ä¼¼çš„å•è¯ã€‚æ­£é¢è¯å¯¹ç›¸ä¼¼æ€§æœ‰ç§¯æè´¡çŒ®ï¼Œè´Ÿé¢è¯æœ‰è´Ÿé¢å½±å“ã€‚

è¯¥æ–¹æ³•è®¡ç®—ç»™å®šå•è¯çš„æŠ•å½±æƒé‡å‘é‡çš„ç®€å•å¹³å‡å€¼ä¸æ¨¡å‹ä¸­æ¯ä¸ªå•è¯çš„å‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚è¯¥æ–¹æ³•å¯¹åº”äºåŸå§‹word2vecå®ç°ä¸­çš„å•è¯ç±»æ¯”å’Œè·ç¦»è„šæœ¬ã€‚

å‚æ•°ï¼š
+ positiveï¼šç§¯æè´¡çŒ®çš„å•è¯åˆ—è¡¨
+ negativeï¼šè´Ÿé¢è´¡çŒ®çš„å•è¯åˆ—è¡¨
+ topnï¼šè¦è¿”å›çš„å‰Nä¸ªç›¸ä¼¼å•è¯çš„æ•°é‡
+ restrct_vocabï¼šå®ƒé™åˆ¶æœç´¢æœ€ç›¸ä¼¼å€¼çš„å‘é‡èŒƒå›´ã€‚ä¾‹å¦‚ï¼Œ+ restrict_vocab = 10000åªä¼šæ£€æŸ¥è¯æ±‡é¡ºåºä¸­çš„å‰10000ä¸ªå•è¯å‘é‡ã€‚ï¼ˆå¦‚æœæ‚¨æŒ‰é™åºé¢‘ç‡å¯¹è¯æ±‡è¡¨è¿›è¡Œæ’åºï¼Œè¿™å¯èƒ½ä¼šæœ‰æ„ä¹‰ã€‚ï¼‰

> most_similar_cosmul(positive=None, negative=None, topn=10)Â¶  

å’Œmost_similarç±»ä¼¼

> n_similarityï¼ˆws1ï¼Œws2 ï¼‰Â¶

è®¡ç®—ä¸¤ç»„å•è¯ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚

> similar_by_vectorï¼ˆvectorï¼Œtopn = 10ï¼Œrestrict_vocab = None ï¼‰Â¶

é€šè¿‡å‘é‡æ‰¾åˆ°å‰Nä¸ªæœ€ç›¸ä¼¼çš„å•è¯ã€‚  
å‚æ•°ï¼š  
+ vecorï¼šè¦è®¡ç®—ç›¸ä¼¼æ€§çš„çŸ¢é‡
+ topnï¼šè¦è¿”å›çš„å‰Nä¸ªç›¸ä¼¼å•è¯çš„æ•°é‡ã€‚å¦‚æœtopnä¸ºFalseï¼Œåˆ™similar_by_vectorè¿”å›ç›¸ä¼¼æ€§å¾—åˆ†çš„å‘é‡ã€‚
+ restrict_vocabï¼šå¯é€‰çš„æ•´æ•°ï¼Œå®ƒé™åˆ¶æœç´¢æœ€ç›¸ä¼¼å€¼çš„å‘é‡èŒƒå›´ã€‚ä¾‹å¦‚ï¼Œrestrict_vocab = 10000åªä¼šæ£€æŸ¥è¯æ±‡é¡ºåºä¸­çš„å‰10000ä¸ªå•è¯å‘é‡ã€‚ï¼ˆå¦‚æœæ‚¨æŒ‰é™åºé¢‘ç‡å¯¹è¯æ±‡è¡¨è¿›è¡Œæ’åºï¼Œè¿™å¯èƒ½ä¼šæœ‰æ„ä¹‰ã€‚ï¼‰

> similar_by_word(word, topn=10, restrict_vocab = None )

é€šè¿‡å•è¯æ‰¾åˆ°å‰Nä¸ªæœ€ç›¸ä¼¼çš„å•è¯ã€‚  

> similarity(w1, w2)

è®¡ç®—ä¸¤ä¸ªå•è¯ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦

> similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100, dtype=<type 'numpy.float32'>)

æ„é€ ç”¨äºè®¡ç®—è½¯ä½™å¼¦æµ‹é‡çš„æœ¯è¯­ç›¸ä¼¼åº¦çŸ©é˜µã€‚

