## 主题和转化 - Topics and Transformations

### 1、转换接口 - Transformation Interface
在上一篇关于语料库和空间向量的教程中，我们创建了一个文档语料库，表示为向量流。要继续，让我们启动gensim并使用该语料库：
```python
from gensim import corpora
from pprint import pprint

dictionary = corpora.Dictionary.load('corpus.dict')
corpus = corpora.MmCorpus('corpus.mm')
pprint(dictionary.token2id)
pprint(list(corpus))

# 结果如下：
{'EPS': 4, '响应': 1, '图': 6, '实验室': 0, '时间': 2, '未成年人': 7, '树': 5, '用户': 3}
[[(0, 1.0)],
 [(1, 1.0), (2, 1.0), (3, 1.0)],
 [(0, 1.0), (4, 1.0)],
 [(0, 1.0), (4, 1.0)],
 [(1, 1.0), (2, 1.0), (3, 1.0)],
 [(5, 1.0)],
 [(6, 1.0)],
 [(5, 1.0), (6, 1.0), (7, 1.0)],
 [(6, 1.0), (7, 1.0)]]
```
在本教程中，我将展示如何将文档从一个矢量表示转换为另一个矢量表示。 这个过程有两个目标：

    1、为了在语料库中显示隐藏的结构，发现单词之间的关系并使用它们以新的（希望）更加语义的方式描述文档。
    2、使文档表示更紧凑。 这既提高了效率（新表示消耗更少的资源）和功效（边际数据趋势被忽略，降噪）。

### 2、创建转换
转化是标准的Python对象，通常通过训练语料库进行初始化
```python
from gensim import models, similarities
# tf-idf 模型
tfidf = models.TfidfModel(corpus)
# 保存tf-idf模型
tfidf.save('corpus.tf-idf.model')
# 加载tf-idf模型
tfidf = models.TfidfModel.load('corpus.tf-idf.model')

for doc in tfidf[corpus]:
    print(doc)
```
其他使用方法
```python
# print tfidf_model.dfs #{单词id，在多少文档中出现}
# # 通过gensim.models.tfidfmodel.df2idf(docfreq, totaldocs, log_base=2.0, add=0.0)方法计算idf值
# # 即idf = add + log(totaldocs / doc_freq),这种算法可能会出现0值
# print tfidf_model.idfs #{单词id，idf值}，
# print tfidf_model.id2word
# print tfidf_model.num_docs #所有文章数目
# print tfidf_model.normalize #是否规范化处理
# print tfidf_model.num_nnz #每个文件中不重复词个数的和4+4 =8
``
我们使用教程1中的旧语料库初始化（训练）转换模型。不同的转换可能需要不同的初始化参数; 在TfIdf的情况下，“训练”仅包括通过提供的语料库一次并计算其所有特征的文档频率。训练其他模型，例如潜在语义分析或潜在Dirichlet分配，涉及更多，因此需要更多时间。

    ⚠️ 注意
    转换总是在两个特定的向量空间之间转换。必须使用相同的向量空间（=同一组特征id）进行训练以及后续的向量转换。无法使用相同的输入要素空间，例如应用不同的字符串预处理，使用不同的特征ID，或使用预期为TfIdf向量的词袋输入向量，将导致转换调用期间的特征不匹配，从而导致垃圾中的任何一个输出和/或运行时异常。


### 3、变换向量
从现在开始，tfidf被视为一个只读对象，可用于将任何向量从旧表示（bag-of-words整数计数）转换为新表示（TfIdf实值权重）：  
举个例子：
```python
docbow = [(0,3),(1,1)]
# 使用该模型来变换矢量
print(tfidf[docbow])

# 结果如下
[(0, 0.9097447007805214), (1, 0.41516813389488305)]
```
也可以转换运用到整个语料库
```python
corpus_idf = tfidf[corpus]
for doc in corpus_idf:
    print(doc)

# 结果如下
[(0, 1.0)]
[(1, 0.5773502691896257), (2, 0.5773502691896257), (3, 0.5773502691896257)]
[(0, 0.5898341626740045), (4, 0.8075244024440723)]
[(0, 0.5898341626740045), (4, 0.8075244024440723)]
[(1, 0.5773502691896257), (2, 0.5773502691896257), (3, 0.5773502691896257)]
[(5, 1.0)]
[(6, 1.0)]
[(5, 0.6282580468670046), (6, 0.45889394536615247), (7, 0.6282580468670046)]
[(6, 0.5898341626740045), (7, 0.8075244024440723)]
```
在这种特殊情况下，我们正在改变我们用于训练的同一语料库，但这只是偶然的。一旦初始化了转换模型，它就可以用在任何向量上（当然它们来自相同的向量空间），即使它们根本没有用在训练语料库中。这是通过LSA的折叠过程，LDA的主题推断等来实现的。

    ⚠️ 注意
    调用model[corpus]仅在旧corpus 文档流周围创建一个包装器- 实际转换在文档迭代期间即时完成。我们无法在调用时转换整个语料库，因为这意味着将结果存储在主存中，这与gensim的内存独立目标相矛盾。如果您将多次迭代转换，并且转换成本很高，请先将生成的语料库序列化为磁盘并继续使用它。corpus_transformed = model[corpus]corpus_transformed

> TF-IDF 转化为 LSI 模型

```python
# 初始化一个LSI变换
lsi = models.LsiModel(corpus_idf, id2word=dictionary, num_topics=2)
# 保存lsi模型
lsi.save('corpus.lsi')
# 加载LSI模型
lsi = models.LsiModel.load('corpus.lsi')
# 创建在原始全集双包装：bow->tfidf->fold-in-lsi
corpus_lsi = lsi[corpus_idf]
topic_lsi = lsi.print_topics(2)
pprint(topic_lsi)

# 结果如下：

[(0,
  '-0.775*"实验室" + -0.632*"EPS" + -0.000*"图" + -0.000*"未成年人" + -0.000*"树" + '
  '-0.000*"响应" + -0.000*"用户" + -0.000*"时间"'),
 (1,
  '-0.705*"图" + -0.552*"未成年人" + -0.445*"树" + 0.000*"实验室" + 0.000*"EPS" + '
  '-0.000*"响应" + -0.000*"用户" + -0.000*"时间"')]
```

LSI培训的独特之处在于我们可以随时继续“培训”，只需提供更多培训文件即可。这是通过在称为在线培训的过程中对底层模型的增量更新来完成的。由于这个特性，输入文档流甚至可能是无限的 - 只需在LSI新文档到达时继续提供它们，同时使用计算的转换模型作为只读！
```python
model.add_documents(another_tfidf_corpus)
lsi_vec = model[tfidf_vec]
```

> 转化为LDA主题模型

LDA（Latent Dirichlet Allocation）是另一种从词袋计数转变为低维度主题空间的转变。LDA是LSA（也称为多项PCA）的概率扩展，因此LDA的主题可以解释为对单词的概率分布。与LSA一样，这些分布也是从训练语料库中自动推断出来的。文档又被解释为这些主题的（软）混合（再次，就像LSA一样）。
```python
corpus = corpora.MmCorpus('corpus.mm')
dictionary = corpora.Dictionary.load('corpus.dict')
model = models.LdaModel(corpus, id2word=dictionary, num_topics=220)
topics = model.print_topics()
print(topics)
```

---

根据上面的学习，我们来看一个完整的例子
[corpus](./corpus.py)