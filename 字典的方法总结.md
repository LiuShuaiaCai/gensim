# 字典的方法总结

下面是详细的步骤
```python
import jieba
from gensim import corpora

# 加载数据
documents_1 = ["我在玉龙雪山","我喜欢玉龙雪山","我还要去玉龙雪山"]
# 分词
text_1 = [ [ word for word in jieba.lcut(document) ] for document in documents_1 ]
# 生成字典,prune_at的作用为控制向量的维数，也就是说最多能为2000000个词语进行向量化
dictionary_1 = corpora.Dictionary(text_1, prune_at=2000000)
# 字典的保存、加载
#dictionary_1.save('corpus.dict')
#corpora.Dictionary.load('corpus.dict')
print(dictionary_1.token2id)

# 对字典进行扩容
documents_2 = ["玉龙雪山很美","大家都说玉龙雪山美","我一定要去玉龙雪山看看"]
text_2 = [ [ word for word in jieba.lcut(document) ] for document in documents_2 ]
dictionary_1.add_documents(text_2, prune_at=2000000)
print(dictionary_1.token2id)

# 字典的合并
#dictionary_2 = corpora.Dictionary(text_2, prune_at=2000000)
#print(dictionary_2.token2id)
#dictionary = dictionary_1.merge_with(dictionary_2)
#print(dictionary.old2new)

# 遍历字典
# dictionary.keys()    #返回所有词语的编号
# dictionary.dfs    #{单词id，在多少文档中出现}
# dictionary.get(5) #返回编号对应的词语，例如这里5->特性。
# dictionary.compactify() #压缩词语向量，如果使用了filter_extremes，filter_n_most_frequent，filter_tokens等删除词典元素的操作，可以使用该方法进行压缩
# dictionary.num_docs #所有文章数目
# dictionary.num_nnz #每个文件中不重复词个数的和
# dictionary.num_pos #所有词的个数
# print(list(dictionary_1.iterkeys()))
# print(list(dictionary_1.iteritems()))
# print(list(dictionary_1.itervalues()))
print('过滤前')
for key in dictionary_1.iterkeys():
    print(key, dictionary_1.get(key),dictionary_1.dfs[key])
    
    
# 过滤字典
# dictionary.filter_extremes(no_below=2, no_above=0.8,keep_n=3 ) #过滤字典,过滤词的出现数量小于2，词频>0.8,且只取前3项
# dictionary.filter_n_most_frequent(2) #过滤出现次数最多的前两个词语
# dictionary.filter_tokens(good_ids=[0]) #good_ids=[0,2]表示仅保留编号为0,2的词语，bad_ids=[1,3]表示要删除编号为1,3的词语
# # 如果想要过滤掉出现次数为1的词，可以使用以下代码
ids = [ id for id in dictionary_1.iterkeys() if dictionary_1.dfs[id] == 1 ]
dictionary_1.filter_tokens(bad_ids=ids)
print('-------------------------过滤后')
for key in dictionary_1.iterkeys():
    print(key, dictionary_1.get(key),dictionary_1.dfs[key])
    
# 向量化
wordstest = "我去玉龙雪山并且喜欢玉龙雪山"
words = [ word for word in jieba.lcut(wordstest) ]
# 将数据向量化doc2bow(document, allow_update=False, return_missing=False)，其实这一步生成了向量化词袋
corpus, missing = dictionary_1.doc2bow(words, return_missing = True)
print(corpus, missing)
# 将test变成词汇编号list
test_id = dictionary_1.doc2idx(words)
print(test_id)

# 语料库的保存/加载
corpora.MmCorpus.serialize('corpus.mm', [corpus])
corpus = corpora.MmCorpus('corpus.mm')

# 语料库的合并
# import itertools
# merged_corpus = itertools.chain(corpus1, corpus2)
print(list(corpus))
```