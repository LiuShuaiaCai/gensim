## 语料库和空间向量 - Corpus and Vector Spaces

#### 可以先设置一下日志的格式
```python
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
```

### 1、从字符串到向量 - From Strings to Vectors
从表示为字符串的文档开始,下面是一个文档列表组成的小型语料库：
```python
documents  =  [ "实验室abc计算机应用程序的人机界面",
              "用户对计算机系统响应时间的意见调查",
              "实验室的EPS用户界面管理系统",
              "实验室的EPS系统与人体系统工程测试",
              "用户感知响应时间与误差测量的关系",
              "随机二进制无序树的生成",
              "树中路径的交叉图",
              "图未成年人IV树的宽度和井准排序",
              "图未成年人调查" ]
```
接下来，对文档进行标记，删除常用的词语以及仅在语料库中出现一次的词语
```python
import jieba
from pprint import pprint

# 停用词
stoplists = set('的 与 和'.split())
# 将语料库分词
texts = [ [ word for word in jieba.lcut(document) if word not in stoplists ] for document in documents ]
# 统计词频
from collections import defaultdict
frequency = defaultdict(int)
for text in texts:
    for word in text:
        frequency[word] += 1
# 删除只出现一次的词语
texts = [ [ token for token in text if frequency[token] > 1 ] for text in texts ]
pprint(texts)

# 结果如下
[['实验室'],
 ['用户', '响应', '时间'],
 ['实验室', 'EPS'],
 ['实验室', 'EPS'],
 ['用户', '响应', '时间'],
 ['树'],
 ['图'],
 ['图', '未成年人', '树'],
 ['图', '未成年人']]
```
将文档转化为向量，我们将使用 [词袋](https://en.wikipedia.org/wiki/Bag-of-words_model) 的文档表示。
> 将字典保存为文件，以便以后可以很好的应用，不用每次都要转化
```python
# 将语料转化为字典
from gensim import corpora
dictionary = corpora.Dictionary(texts)
# 保存字典
dictionary.save('corpus.dict')
```
在这里，我们为语料库中出现的所有单词分配了一个唯一的整数id gensim.corpora.dictionary.Dictionary。这会扫描文本，收集字数和相关统计数据。最后，我们看到在处理过的语料库中有12个不同的单词，这意味着每个文档将由12个数字表示（即，通过12-D向量）。要查看单词及其ID之间的映射：
```python
# 加载字典模型
dictionary = corpora.Dictionary.load('corpus.dict')
# 查看单词及其ID之间的映射
mapping = dictionary.token2id
pprint(mapping)

# 结果如下
{'EPS': 4, '响应': 1, '图': 6, '实验室': 0, '时间': 2, '未成年人': 7, '树': 5, '用户': 3}
```

将标记化文档实际转换为向量：
> 我们先看一个小栗子
```python
new_doc = "实验室用户"
new_vec = dictionary.doc2bow(jieba.lcut(new_doc))
pprint(new_vec)
# 结果如下
[(0, 1), (3, 1)]
```

    注：函数doc2bow()只计算每个不同单词的出现次数，将单词转换为整数单词id，并将结果作为稀疏向量返回

将语料库转化为向量，并保存
```python
corpus = [ dictionary.doc2bow(text) for text in texts ]
# 保存为向量文本
corpora.MmCorpus.serialize('corpus.mm', corpus)
pprint(corpus)
# 加载语料文本向量
corpus = corpora.MmCorpus('corpus.mm')

# 结果如下
[[(0, 1)],
 [(1, 1), (2, 1), (3, 1)],
 [(0, 1), (4, 1)],
 [(0, 1), (4, 1)],
 [(1, 1), (2, 1), (3, 1)],
 [(5, 1)],
 [(6, 1)],
 [(5, 1), (6, 1), (7, 1)],
 [(6, 1), (7, 1)]]
```

### 2、语料库流 - 一次一个文档
请注意，上面的语料库完全驻留在内存中，作为普通的Python列表。在这个简单的例子中，它并不重要，但为了使事情清楚，让我们假设语料库中有数百万个文档。将所有这些存储在RAM中是行不通的。相反，我们假设文档存储在磁盘上的文件中，每行一个文档。Gensim只要求语料库必须能够一次返回一个文档向量：

```python
class MyCorpus:
    def __iter__(self):
        for line in open('corpus.txt'):
            yield dictionary.doc2bow(jieba.lcut(line))
corpus = MyCorpus()
pprint(list(corpus))

# 结果如下
[[(0, 1)],
 [(1, 1), (2, 1), (3, 1)],
 [(0, 1), (4, 1)],
 [(0, 1), (4, 1)],
 [(1, 1), (2, 1), (3, 1)],
 [(5, 1)],
 [(6, 1)],
 [(5, 1), (6, 1), (7, 1)],
 [(6, 1), (7, 1)]]
```
尽管输出与普通Python列表的输出相同，但语料库现在更加内存友好，因为一次最多只有一个向量驻留在RAM中。您的语料库现在可以随意扩展。

类似地，构造字典而不将所有文本加载到内存中：
```python
# 所有词语的统计信息
dictionary = corpora.Dictionary(jieba.lcut(line) for line in open('corpus.txt', encoding='utf-8'))
# 除去停止词和仅出现一次的话
stop_ids = [ dictionary.token2id[stopword] for stopword in stoplists if stopword in dictionary.token2id ]
once_ids = [ tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1 ]
dictionary.filter_tokens(stop_ids + once_ids)
# 删除被删除的单词后的id序列中的空格,但是空格并没有去掉
dictionary.compactify()
pprint(dictionary.token2id)
```

### 3、与 Numpy 和 SciPy 的兼容性
Gensim还包含有效的实用程序函数 来帮助numpy矩阵的转化
```python
import gensim
import numpy as np
np_matrix = np.random.randint(10, size=[5,2])
corpus = gensim.matutils.Dense2Corpus(np_matrix)
np_matrix = gensim.matutils.corpus2dense(corpus, num_terms = 5)
print(np_matrix)
```
scipy.sparse矩阵相互转化
```python
import scipy.sparse
ss_matrix = scipy.sparse.random(5,2,density=1)
corpus = gensim.matutils.Sparse2Corpus(ss_matrix)
print(list(corpus))
csc_matrix = gensim.matutils.corpus2csc(corpus)
print(list(csc_matrix))
```
